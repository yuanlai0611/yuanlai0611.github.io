<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Jsoup 模拟登录]]></title>
    <url>%2F2018%2F04%2F23%2FJsoup-%E6%A8%A1%E6%8B%9F%E7%99%BB%E5%BD%95%2F</url>
    <content type="text"><![CDATA[在日常开发的时候，我们爬取数据，有些数据需要我们登陆之后才能够爬取。举一个例子，我们想要爬取学校图书馆里面的历史图书，我们就必须登陆以后再获取，这个时候我们可以选择模拟登陆——即我们可以用程序来模拟整个登陆的过程。 Jsoup模拟登陆的大概步骤(以待验证码的登陆为例) 首先对整个登陆过程进行网络分析，搞清楚需要用到哪些参数，以及请求的网址。 输入参数或者在网页中爬取相应的参数。 对请求网址发起网络请求，获取返回的数据，一般情况下是一个网页。 登录成功以后就可以通过获取之前网络请求的cookie来访问一些需要登录后才能浏览的网页 Jsoup模拟登陆的详细步骤 我们想要模拟登陆就必须了解整个登陆的完整过程，这里我推荐使用Google的Chrome浏览器来分析整个网络请求的过程。先用Chrome浏览器加载出登陆的页面，然后点反键选择检查，接着选择network。 按照提示刷新页面，mac使用command加R刷新后才能看到网络请求。我们先试着登录一遍，填好了所有的数据以后登录，我们可以看到我这个请求的所有请求参数可以在Headers里面的Form Data里面看到，下面的例子中我们一共看到了五个参数：1. number 2.passwd 3.captcha 4.select 5.returnUrl 同时在Headers里面的General可以看到登陆的请求网址。 这样我们需要的请求网址和所需的所有参数都知道了。 通过上面的一个步骤我们知道我们的所有参数里面有一个验证码需要我们获取，我的思路是先把他下载到本地再输入，那这个验证码如何获取呢？我们可以在所有的网络请求的列表里面看到验证码的请求，并且得到验证码的请求地址。 我们先向验证码的地址发送请求，获取到回应是二进制数组，再下载到本地。这里还要补充一句，我所有的网络请求和爬取数据用的都是Jsoup，需要在网络上面下载Jsoup的Jar包再添加到项目的lib里面，这里具体的做法我就不做阐述，具体的可以自己在网上搜索，这里可以参考Eclipse的三个方法，我是用的是外部添加Jar的方法。具体的代码如下： 12345678910111213141516171819202122Response response = Jsoup.connect("http://opac.lib.wust.edu.cn:8080/reader/captcha.php") .ignoreContentType(true) .execute(); byte[] data = response.bodyAsBytes(); cookie = response.cookies(); if (data != null) &#123; String filepath = "/Users/yuanyuanlai/Pictures"+"\\"+"yzm.gif"; File file = new File(filepath); if (file.exists()) &#123; file.delete(); &#125; try &#123; FileOutputStream fos = new FileOutputStream(file); fos.write(data, 0, data.length); fos.flush(); fos.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; 这里我们一定要注意上面的一个获取cookie的这个步骤，因为这个cookie联系整个步骤，要不然验证码怎么和后面的登录请求联系到一起 这一步里我们可以正式发送请求，具体的代码如下： 1234567891011121314151617181920System.out.println("请输入验证码:"); Scanner input = new Scanner(System.in); String yzm = input.next(); input.close(); Map&lt;String, String&gt; datas = new HashMap&lt;String,String&gt;(); datas.put("number", "201613137217"); datas.put("passwd", "310810"); datas.put("captcha", yzm); datas.put("select", "bar_no"); datas.put("returnUrl", ""); Connection connection2 = Jsoup.connect("http://opac.lib.wust.edu.cn:8080/reader/redr_verify.php"); connection2.header("User-Agent", "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.13; rv:57.0) Gecko/20100101 Firefox/57.0"); res = connection2 .data(datas) .cookies(cookie) .method(Method.POST) .execute(); 将之前获取的验证码输入进去，以Map的形式储存请求的参数，简单的设置浏览器代理以后发送请求，这里最为重要的是在发送请求的时候也要带上cookie。 完成登录之后我们可以尝试着使用登录成功后的cookie爬取需要登录后的网页，比如我登录成功后想要爬取历史图书，比如下图： 我们在代码里面尝试着登录，爬取到历史图书，具体代码如下： 12345678document = Jsoup.connect("http://opac.lib.wust.edu.cn:8080/reader/book_hist.php") .cookies(cookie) .get(); Elements elements = document.select("table.table_line").select("tbody") .select("tr"); for(int i=1 ; i&lt;elements.size() ;i++) &#123; System.out.println(elements.get(i).select("td").get(2).text()); &#125; 爬取到的结果是： 说明登录成功，这里还得补充一句，Jsoup的具体教程得参考它的官方教程Jsoup教程 所有的代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384import java.io.File;import java.io.FileOutputStream;import java.io.IOException;import java.util.HashMap;import java.util.Map;import java.util.Scanner;import org.jsoup.Connection;import org.jsoup.Connection.Method;import org.jsoup.Connection.Response;import org.jsoup.Jsoup;import org.jsoup.nodes.Document;import org.jsoup.select.Elements;public class loginTest1 &#123; public static void main(String[] args) &#123; // TODO Auto-generated method stub Document document = null; Response res = null; Document doc = null; Map&lt;String, String&gt; cookie = new HashMap&lt;String,String&gt;(); try &#123; Response response = Jsoup.connect("http://opac.lib.wust.edu.cn:8080/reader/captcha.php") .ignoreContentType(true) .execute(); byte[] data = response.bodyAsBytes(); cookie = response.cookies(); if (data != null) &#123; String filepath = "/Users/yuanyuanlai/Pictures"+"\\"+"yzm.gif"; File file = new File(filepath); if (file.exists()) &#123; file.delete(); &#125; try &#123; FileOutputStream fos = new FileOutputStream(file); fos.write(data, 0, data.length); fos.flush(); fos.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; System.out.println("请输入验证码:"); Scanner input = new Scanner(System.in); String yzm = input.next(); input.close(); Map&lt;String, String&gt; datas = new HashMap&lt;String,String&gt;(); datas.put("number", "201613137217"); datas.put("passwd", "310810"); datas.put("captcha", yzm); datas.put("select", "bar_no"); datas.put("returnUrl", ""); Connection connection2 = Jsoup.connect("http://opac.lib.wust.edu.cn:8080/reader/redr_verify.php"); connection2.header("User-Agent", "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.13; rv:57.0) Gecko/20100101 Firefox/57.0"); res = connection2 .data(datas) .cookies(cookie) .method(Method.POST) .execute(); document = Jsoup.connect("http://opac.lib.wust.edu.cn:8080/reader/book_hist.php") .cookies(cookie) .get(); Elements elements = document.select("table.table_line").select("tbody") .select("tr"); for(int i=1 ; i&lt;elements.size() ;i++) &#123; System.out.println(elements.get(i).select("td").get(2).text()); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125;]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>模拟登录</tag>
        <tag>Jsoup</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[稳稳的科学上网教程]]></title>
    <url>%2F2018%2F04%2F23%2F%E7%A8%B3%E7%A8%B3%E7%9A%84%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[服务器选择Centos 6x64 锐速更换内核： 1rpm -ivh http://soft.91yun.org/ISO/Linux/CentOS/kernel/kernel-firmware-2.6.32-504.3.3.el6.noarch.rpm 1rpm -ivh http://soft.91yun.org/ISO/Linux/CentOS/kernel/kernel-2.6.32-504.3.3.el6.x86_64.rpm --force reboot 1wget -N --no-check-certificate https://softs.fun/Bash/ssr.sh &amp;&amp; chmod +x ssr.sh &amp;&amp; bash ssr.sh 安装锐速 安装libsodium(chacha20) 安装ShadowSocksR 配置后的相关信息：相关链接：服务器网址，价格和可靠性都不错ping测试网站，用来检测ip地址是否被黑]]></content>
      <categories>
        <category>科学上网</category>
      </categories>
      <tags>
        <tag>实用工具</tag>
      </tags>
  </entry>
</search>
